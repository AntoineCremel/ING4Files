{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning assignment week 5\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jacques', 'a', 'dit']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a dit</td>\n",
       "      <td>Jacques</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message   Target\n",
       "0   a dit  Jacques"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for the organisation of the data set\n",
    "words = \"Jacques a dit\".split(\" \")\n",
    "print(words)\n",
    "df = pd.DataFrame(data=[[' '.join(words[1:]), words[0]]],\\\n",
    "                  columns=[\"message\", \"Target\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      "Message    5000 non-null object\n",
      "Target     5000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 117.2+ KB\n",
      "None\n",
      "                                             Message Target\n",
      "0                   Yup i've finished c Ã¼ there...\\n    ham\n",
      "1             Remember to ask alex about his pizza\\n    ham\n",
      "2                     No da..today also i forgot..\\n    ham\n",
      "3  Ola would get back to you maybe not today but ...    ham\n",
      "4  Fwiw the reason I'm only around when it's time...    ham\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"Message\", \"Target\"])\n",
    "\n",
    "counter = 0\n",
    "with open(\"messages.txt\") as messages_file:\n",
    "    for line in messages_file:\n",
    "        words = line.split(\"\\t\")\n",
    "        data.loc[counter] = [words[1], words[0]]\n",
    "        counter += 1\n",
    "        \n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate between train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3999 entries, 0 to 3998\n",
      "Data columns (total 2 columns):\n",
      "Message    3999 non-null object\n",
      "Target     3999 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 62.6+ KB\n",
      "Train :  None\n",
      "                                             Message Target\n",
      "0  Dorothy@kiefer.com (Bank of Granite issues Str...   spam\n",
      "1  says the  &lt;#&gt;  year old with a man and m...    ham\n",
      "2                       I will come to ur home now\\n    ham\n",
      "3  Free any day but i finish at 6 on mon n thurs....    ham\n",
      "4                        Will you be here for food\\n    ham\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1001 entries, 0 to 1000\n",
      "Data columns (total 2 columns):\n",
      "Message    1001 non-null object\n",
      "Target     1001 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.7+ KB\n",
      "Test :  None\n"
     ]
    }
   ],
   "source": [
    "train_proportion = 0.8\n",
    "\n",
    "\n",
    "train_data = data[1 - int(data.shape[0] * train_proportion):].reset_index(drop=True)\n",
    "test_data = data[:1-int(data.shape[0] * train_proportion)].reset_index(drop=True)\n",
    "print(\"Train : \", train_data.info())\n",
    "print(train_data.head())\n",
    "print(\"Test : \", test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_dictionnary(data, most_commons):\n",
    "    \"\"\"\n",
    "        Data is a pandas DataFrame generated earlier\n",
    "        most_common represents the number of most common words we want to take\n",
    "    \"\"\"\n",
    "    # Generate a list of words\n",
    "    word_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        message = data.at[i, \"Message\"]\n",
    "        # To add : remove punctuation from message\n",
    "        words = message.split(\" \")\n",
    "        \n",
    "        word_list += words\n",
    "        \n",
    "    word_dic = Counter(word_list)\n",
    "    for item in list(word_dic):\n",
    "        if item.isalpha() == False or len(item) == 1:\n",
    "            del word_dic[item]\n",
    "            \n",
    "    return word_dic.most_common(most_commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_size = 3000\n",
    "\n",
    "words_dic = make_dictionnary(train_data, dic_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the data sets\n",
    "#### Turn Spam and ham into 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Message  Target\n",
      "0  Dorothy@kiefer.com (Bank of Granite issues Str...       1\n",
      "1  says the  &lt;#&gt;  year old with a man and m...       0\n",
      "2                       I will come to ur home now\\n       0\n",
      "3  Free any day but i finish at 6 on mon n thurs....       0\n",
      "4                        Will you be here for food\\n       0\n"
     ]
    }
   ],
   "source": [
    "def parse_target(data):\n",
    "    data[\"Target\"] = pd.Categorical(data[\"Target\"]).codes\n",
    "    return data\n",
    "    \n",
    "print(parse_target(train_data).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn each message into a vector by using the dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, words_dic):\n",
    "    feature_matrix = np.zeros((data.shape[0], len(words_dic)))\n",
    "    \n",
    "    messageID = 0\n",
    "    for line in data[\"Message\"]:\n",
    "        words = line.split(\" \")\n",
    "        for word in words:\n",
    "            for i, d in enumerate(words_dic):\n",
    "                if d[0] == word:\n",
    "                    feature_matrix[messageID, i] += 1\n",
    "        messageID += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "train_features = extract_features(train_data, words_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Naive Bayes\n",
    "\n",
    "For this part, we will use the Bayes formula.\n",
    "* For each message, we write p(spam) the probability that this message is a spam, and p(x0) the probability that the word 0 is in that message.\n",
    "* p(spam | x0) = (p(x0 | spam) * p(spam)) / p(x0).\n",
    "* We know p(x0) and p(spam) trivially by counting how many instances of each are in our training set, and we can find p(x0 | spam) by looking for each word at how often they appear in spams.\n",
    "\n",
    "We thus have 3 steps to train a naive Bayes classifier for our spam filter :\n",
    "* Find p(X), for each word, the probability it is in a message.\n",
    "* Find p(spam), for each message, the probability it is a spam.\n",
    "* Find p(X | spam) : for each word, the probability it is in a spam.\n",
    "\n",
    "##### Find P(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13703425856464116\n"
     ]
    }
   ],
   "source": [
    "p_spam = train_data[\"Target\"][train_data[\"Target\"] == 1].count() / train_data.shape[0]\n",
    "print(p_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find p(X)\n",
    "* For each word, find the probability that it is x amount of time in any given message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our p_X.\n",
    "# It is a matrix with each word as a line and the amount of time it appears\n",
    "# in a message as a column\n",
    "\n",
    "def count_nb_instances_for_each_word(data, words_dic, max_amount=5):\n",
    "    # Variables\n",
    "    retour = np.zeros((len(words_dic), max_amount))\n",
    "    \n",
    "    # Loop through each word of the dictionnary\n",
    "    for i, dic_word in enumerate(words_dic):\n",
    "        # Loop through each message\n",
    "        for message in data[\"Message\"]:\n",
    "            instance_count = 0\n",
    "            words = message.split(\" \")\n",
    "            \n",
    "            # Loop tjrough each word of the message\n",
    "            for word in words:\n",
    "                if word == dic_word[0]:\n",
    "                    instance_count += 1\n",
    "            if instance_count >= max_amount:\n",
    "                instance_count = max_amount - 1\n",
    "            retour[i, instance_count] += 1\n",
    "    \n",
    "    # Turn the matrice into probabi p\n",
    "    retour /= np.sum(retour[0, :])\n",
    "    \n",
    "    return retour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the above function to find the number of instances of each word in all messages in training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5)\n"
     ]
    }
   ],
   "source": [
    "p_X = count_nb_instances_for_each_word(train_data, words_dic)\n",
    "\n",
    "print(p_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70667667 0.22455614 0.05151288 0.01250313 0.00475119]\n"
     ]
    }
   ],
   "source": [
    "print(p_X[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5)\n",
      "[0.4379562  0.3850365  0.12591241 0.04562044 0.00547445]\n"
     ]
    }
   ],
   "source": [
    "p_X_spam = count_nb_instances_for_each_word(\\\n",
    "            train_data[train_data[\"Target\"] == 1], words_dic)\n",
    "\n",
    "print(p_X_spam.shape)\n",
    "print(p_X_spam[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70667667 0.22455614 0.05151288 0.01250313 0.00475119]\n"
     ]
    }
   ],
   "source": [
    "np.sum(p_X[0, :])\n",
    "print((p_X/ np.sum(p_X[0, :]))[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit maintenant d'appliquer la formule grÃ¢ce Ã  np.apply_formula.plz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5)\n",
      "[0.08492569 0.23496658 0.33495139 0.4999996  0.1578944 ]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.00000001\n",
    "# We now have all the elements to give to each word a probability\n",
    "p_spam_X = np.divide(p_X_spam * p_spam, p_X + epsilon)\n",
    "print(p_spam_X.shape)\n",
    "print(p_spam_X[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
